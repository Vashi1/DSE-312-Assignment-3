{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80538b0925b4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce9bacb29fb00eb2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import vq\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:01.625798146Z",
     "start_time": "2023-10-29T06:28:01.584585909Z"
    }
   },
   "id": "e2df8048be3f9bc3"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('cifar10', split=['train', 'test'], shuffle_files=True, with_info=True, as_supervised=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:28.719138184Z",
     "start_time": "2023-10-29T06:28:28.665372820Z"
    }
   },
   "id": "1af40dc1b69f4883"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 11:58:40.514124: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train_images, train_labels \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mas_numpy(ds_train)\n\u001B[1;32m      2\u001B[0m test_images, test_labels \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mas_numpy(ds_test)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = tfds.as_numpy(ds_train)\n",
    "test_images, test_labels = tfds.as_numpy(ds_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:40.525602191Z",
     "start_time": "2023-10-29T06:28:40.476177853Z"
    }
   },
   "id": "70d88639dc51b361"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalizing the Image Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9625cd878145e3ae"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "X_train, X_test = train_images, test_images\n",
    "y_train, y_test = train_labels, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:01.682173488Z",
     "start_time": "2023-10-29T06:28:01.638543165Z"
    }
   },
   "id": "105cd8f51ca19a2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Subpart 1**\\\n",
    "*SIFT + SVM*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a22ce597557e3e7d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def extract_sift_features(data):\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Extract SIFT features from the dataset\n",
    "    sift_features = []\n",
    "\n",
    "    for image in ds:\n",
    "        # Convert the image to grayscale (SIFT works on grayscale images)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Detect SIFT keypoints and compute descriptors\n",
    "        kp, descriptors = sift.detectAndCompute(gray_image, None)\n",
    "\n",
    "        # Store SIFT descriptors in the list\n",
    "        sift_features.append(descriptors)\n",
    "\n",
    "    return sift_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:01.682334380Z",
     "start_time": "2023-10-29T06:28:01.681261186Z"
    }
   },
   "id": "3239e0223f42a7bd"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<tensorflow_datasets.core.dataset_utils._IterableDataset object at 0x7f1748202310>,\n      dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m xtr, xte, ytr, yte \u001B[38;5;241m=\u001B[39m train_test_split(X_train, y_train, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)    \n\u001B[1;32m      2\u001B[0m sift_features \u001B[38;5;241m=\u001B[39m extract_sift_features(xtr)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    212\u001B[0m         )\n\u001B[1;32m    213\u001B[0m     ):\n\u001B[0;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    224\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2646\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2643\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_arrays \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   2644\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one array required as input\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 2646\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[1;32m   2648\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m   2649\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m _validate_shuffle_split(\n\u001B[1;32m   2650\u001B[0m     n_samples, test_size, train_size, default_test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m\n\u001B[1;32m   2651\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:453\u001B[0m, in \u001B[0;36mindexable\u001B[0;34m(*iterables)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001B[39;00m\n\u001B[1;32m    435\u001B[0m \n\u001B[1;32m    436\u001B[0m \u001B[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001B[39;00m\n\u001B[1;32m    450\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    452\u001B[0m result \u001B[38;5;241m=\u001B[39m [_make_indexable(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m iterables]\n\u001B[0;32m--> 453\u001B[0m check_consistent_length(\u001B[38;5;241m*\u001B[39mresult)\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:404\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_consistent_length\u001B[39m(\u001B[38;5;241m*\u001B[39marrays):\n\u001B[1;32m    394\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001B[39;00m\n\u001B[1;32m    395\u001B[0m \n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;124;03m        Objects that will be checked for consistent length.\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 404\u001B[0m     lengths \u001B[38;5;241m=\u001B[39m [_num_samples(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m arrays \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m    405\u001B[0m     uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:404\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_consistent_length\u001B[39m(\u001B[38;5;241m*\u001B[39marrays):\n\u001B[1;32m    394\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that all arrays have consistent first dimensions.\u001B[39;00m\n\u001B[1;32m    395\u001B[0m \n\u001B[1;32m    396\u001B[0m \u001B[38;5;124;03m    Checks whether all objects in arrays have the same shape or length.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;124;03m        Objects that will be checked for consistent length.\u001B[39;00m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 404\u001B[0m     lengths \u001B[38;5;241m=\u001B[39m [_num_samples(X) \u001B[38;5;28;01mfor\u001B[39;00m X \u001B[38;5;129;01min\u001B[39;00m arrays \u001B[38;5;28;01mif\u001B[39;00m X \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m    405\u001B[0m     uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:345\u001B[0m, in \u001B[0;36m_num_samples\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 345\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    346\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingleton array \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m cannot be considered a valid collection.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m x\n\u001B[1;32m    347\u001B[0m         )\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;66;03m# Check that shape is returning an integer or default to len\u001B[39;00m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001B[39;00m\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], numbers\u001B[38;5;241m.\u001B[39mIntegral):\n",
      "\u001B[0;31mTypeError\u001B[0m: Singleton array array(<tensorflow_datasets.core.dataset_utils._IterableDataset object at 0x7f1748202310>,\n      dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "xtr, xte, ytr, yte = train_test_split(X_train, y_train, test_size=0.2, random_state=42)    \n",
    "sift_features = extract_sift_features(xtr)\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(sift_features, ytr)\n",
    "# y_pred = clf.predict(extract_sift_features(xte))\n",
    "\n",
    "# Calculate accuracy\n",
    "# accuracy = accuracy_score(yte   , y_pred)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-29T06:28:01.869404314Z",
     "start_time": "2023-10-29T06:28:01.681479077Z"
    }
   },
   "id": "f8ed515e6752f980"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# \n",
    "# print(classification_report(yte, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-29T06:28:01.869316064Z"
    }
   },
   "id": "701a79bf448b07ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# \n",
    "# conf_mat = confusion_matrix(yte, y_pred)\n",
    "# sns.heatmap(conf_mat, annot = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-29T06:28:01.869386736Z"
    }
   },
   "id": "8bfc653b51fe0e1a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
