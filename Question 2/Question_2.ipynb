{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 22:37:54.085795: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-04 22:37:54.146004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-04 22:37:54.146054: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-04 22:37:54.146106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-04 22:37:54.156853: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-04 22:37:54.157503: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-04 22:37:55.786822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-04 22:37:58.778623: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = tfds.load('cifar10', split=['train','test'], as_supervised=True, batch_size = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = tfds.as_numpy(train_ds)\n",
    "test_images, test_labels = tfds.as_numpy(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying LBP on the Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_value(img, center, x, y):\n",
    "    try:\n",
    "        if (img[x, y] >= center).all():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except IndexError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lbp_value(img, x, y):\n",
    "    center = img[x, y]\n",
    "    binary_values = []\n",
    "    \n",
    "    binary_values.append(get_pixel_value(img, center, x-1, y-1))\n",
    "    binary_values.append(get_pixel_value(img, center, x-1, y))\n",
    "    binary_values.append(get_pixel_value(img, center, x-1, y+1))\n",
    "    binary_values.append(get_pixel_value(img, center, x, y+1))\n",
    "    binary_values.append(get_pixel_value(img, center, x+1, y+1))\n",
    "    binary_values.append(get_pixel_value(img, center, x+1, y))\n",
    "    binary_values.append(get_pixel_value(img, center, x+1, y-1))\n",
    "    binary_values.append(get_pixel_value(img, center, x, y-1))\n",
    "    \n",
    "    lbp_value = 0\n",
    "    for i in range(8):\n",
    "        lbp_value += binary_values[i] * (2**i)\n",
    "    \n",
    "    return lbp_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lbp_image(img):\n",
    "    height, width, _ = img.shape\n",
    "    lbp_img = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    for x in range(1, height-1):\n",
    "        for y in range(1, width-1):\n",
    "            lbp_img[x, y] = calculate_lbp_value(img, x, y)\n",
    "    \n",
    "    return lbp_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 49999 is out of bounds for axis 0 with size 10000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question 2/Question_2.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question%202/Question_2.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test_images_lbp \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question%202/Question_2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(test_images)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question%202/Question_2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     test_images_lbp\u001b[39m.\u001b[39mappend(calculate_lbp_image(test_images[i]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question%202/Question_2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_images_lbp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(test_images_lbp)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/aarvee/PycharmProjects/DSE-312-Assignment-3/Question%202/Question_2.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_images_lbp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(train_images_lbp)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 49999 is out of bounds for axis 0 with size 10000"
     ]
    }
   ],
   "source": [
    "train_images_lbp = []\n",
    "for i in range(0, len(train_images)):\n",
    "    train_images_lbp.append(calculate_lbp_image(train_images[i]))\n",
    "\n",
    "test_images_lbp = []\n",
    "for a in range(0, len(test_images)):\n",
    "    test_images_lbp.append(calculate_lbp_image(test_images[a]))\n",
    "\n",
    "test_images_lbp = np.array(test_images_lbp)\n",
    "train_images_lbp = np.array(train_images_lbp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(images_list):\n",
    "    image_descriptor = []\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    for image in images_list:\n",
    "        _, descriptor = sift.detectAndCompute(image, None)\n",
    "        image_descriptor.append(descriptor)\n",
    "    \n",
    "    return image_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmean_bow(all_descrip, num_clusters):\n",
    "    bow_dict = []\n",
    "    kmeans = KMeans(n_clusters = num_clusters)\n",
    "    kmeans.fit(all_descrip)\n",
    "    bow_dict = kmeans.cluster_centers_\n",
    "    return bow_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_bow(image_desc, BoW, num_cluster):\n",
    "    X_features = []\n",
    "    for i in range(len(image_desc)):\n",
    "        features = np.array([0] * num_cluster)\n",
    "        if image_desc[i] is not None:\n",
    "            distance = cdist(image_desc[i], BoW)\n",
    "            argmin = np.argmin(distance, axis = 1)\n",
    "            for j in argmin:\n",
    "                features[j] += 1\n",
    "        X_features.append(features)\n",
    "    \n",
    "    return X_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying SIFT on Train Images and Creating Bag of Words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@162.205] global shadow_sift.hpp:13 SIFT_create DEPRECATED: cv.xfeatures2d.SIFT_create() is deprecated due SIFT tranfer to the main repository. https://github.com/opencv/opencv/issues/16736\n",
      "/home/aarvee/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'sklearn.cluster._k_means_common._relocate_empty_clusters_dense'\n",
      "Traceback (most recent call last):\n",
      "  File \"<__array_function__ internals>\", line 177, in where\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "image_descriptors = extract_sift_features(train_images_lbp)\n",
    "all_descriptors = []\n",
    "for desciptor in image_descriptors:\n",
    "    if desciptor is not None:\n",
    "        for des in desciptor:\n",
    "            all_descriptors.append(des)\n",
    "\n",
    "num_cluster = 60\n",
    "BoW = kmean_bow(all_descriptors, num_cluster)\n",
    "X_features = create_feature_bow(image_descriptors, BoW, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Applying SIFT on Test Images and Creating Bag of Words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_descriptors = extract_sift_features(test_images_lbp)\n",
    "all_descriptors = []\n",
    "for descriptor in image_descriptors:\n",
    "    if descriptor is not None:\n",
    "        for des in descriptor:\n",
    "            all_descriptors.append(des)\n",
    "\n",
    "num_cluster = 60\n",
    "BoW = kmean_bow(all_descriptors, num_cluster)\n",
    "Y_features = create_feature_bow(image_descriptors, BoW, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing SVM on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "model_svm = SVC(C = 30, random_state=42)\n",
    "model_svm.fit(X_features, Y_features)\n",
    "pred_images = model_svm.predict(test_images)\n",
    "print(\"Confustion Report: \")\n",
    "print(classification_report(pred_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(test_images, pred_images)\n",
    "sns.heatmap(conf_mat, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extracting HoG Features*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(X):\n",
    "    features = []\n",
    "    for image in X:\n",
    "        # Convert image to grayscale\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Ensure the image has the correct data type (CV_8U)\n",
    "        gray_image = np.uint8(gray_image * 255)\n",
    "\n",
    "        # Calculate HOG features\n",
    "        fd = hog(gray_image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "        features.append(fd)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hog = extract_hog_features(train_images)\n",
    "y_test_hog = extract_hog_features(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_hog = scaler.fit_transform(X_train_hog)\n",
    "y_test_hog = scaler.transform(y_test_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hog = SVC(C = 30, random_state=42)\n",
    "clf_hog.fit(X_train_hog, train_labels)\n",
    "\n",
    "pred_hog = clf_hog.predict(y_test_hog)\n",
    "print(classification_report(test_labels, pred_hog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(test_labels, pred_hog)\n",
    "sns.heatmap(conf_mat, annot = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
